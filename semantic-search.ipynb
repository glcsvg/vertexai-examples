{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "\n",
    "# credentials.json dosyasının yolu\n",
    "credentials_path =  \"tough-history5d2.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'tough-history-431913'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=REGION, credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\n",
    "    \"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/text1.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = text.strip().split(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk Türk astronot Gezeravcı'nın da yer aldığı Ax-3 ekibi Uluslararası Uzay İstasyonu'nda\n",
      "..\n",
      "Türk, İspanyol-Amerikan, İtalyan ve İsveçli olmak üzere çok uluslu Ax-3 ekibini taşıyan Dragon kapsülünün alçak dünya yörüngesindeki laboratuvar olarak tanımlanan UUİ'ye kenetlenme anı, Axiom Space'in internet sitesinden canlı yayınlandı.\n",
      "Yaklaşık 36 saat süren uzay yolculuğunun ardından ilk Türk astronot Gezeravcı'nın da yer aldığı Ax-3 ekibi, ABD yerel saatiyle sabah 05.42 (TSİ 13.42) civarında UUİ'ye ulaşt\n",
      "UUİ'de bulunan, misyonları 27 Eylül 2023'te başlayan Expedition 70 astronot ve kozmonotları , Ax-3 ekibini, istasyonda karşıladı\n",
      "Gezeravcı, 2 hafta boyunca uzayda bilimsel çalışmalar yapacak\n",
      "Gezeravcı, 14 gün boyunca kalacağı UUİ'de 13 farklı bilimsel deney üzerinde çalışacak.\n",
      "Bu deneyler, mikro yer çekimi, uzay ortamında insan sağlığı, Tuz Gölü bitkisinin uzay ortamında araştırılması, katı-akışkan karışımların yerçekimsiz ortamda araştırılması gibi alanlarda çeşitlilik gösteriyor.\n",
      "..\n",
      "Zeynep Katre Oran, Irmak Akcan  \n",
      "Gündem-Dünya 2024.01.20\n",
      "..\n",
      "\n",
      "Ordu'nun Fatsa ilçesinde şiddetli yağış su baskınlarına neden oldu.\n",
      "..\n",
      "Ordu'nun Fatsa ilçesinde etkili olan şiddetli yağış nedeniyle bazı ev ile iş yerlerini su bastı.İlçede aralıklarla süren sağanak kısa sürede şiddetini artırdı. Yağışla birlikte ilçedeki bazı yollarda ve caddelerde su birikintileri oluştu.\n",
      "Yağış nedeniyle ilçedeki bazı iş yerleri ile evlerin bodrum katlarını su basarken, Karadeniz Sahil Yolu'nda da etkili olan yağış nedeniyle araçlar ilerlemekte zorluk yaşadı.\n",
      "..\n",
      "\n",
      "Hayati Akçay \n",
      "Gündem 08.07.2024\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "for i in news:\n",
    "    print(i)\n",
    "    print(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "\n",
    "for i in range(0,len(news),3):\n",
    "    headline = news[i].strip()\n",
    "    content = news[i+1].strip()\n",
    "    category_author_date = news[i+2].strip()\n",
    "\n",
    "    category_author = category_author_date.strip().split(\"\\n\")\n",
    "    author = category_author[0].strip()\n",
    "    cat_date = category_author[1].split(' ')\n",
    "    date = cat_date[1].strip()\n",
    "    category = cat_date[0].strip()\n",
    "\n",
    "    data.append([headline,content,category,author,date])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "import functools\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_batches(sentences, batch_size = 5):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "def encode_texts_to_embeddings(sentences):\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]\n",
    "    \n",
    "def encode_text_to_embedding_batched(sentences, api_calls_per_second = 0.33, batch_size = 5):\n",
    "    # Generates batches and calls embedding API\n",
    "    \n",
    "    embeddings_list = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total = math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return embeddings_list_successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>İlk Türk astronot Gezeravcı'nın da yer aldığı ...</td>\n",
       "      <td>Türk, İspanyol-Amerikan, İtalyan ve İsveçli ol...</td>\n",
       "      <td>Gündem-Dünya</td>\n",
       "      <td>Zeynep Katre Oran, Irmak Akcan</td>\n",
       "      <td>2024.01.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ordu'nun Fatsa ilçesinde şiddetli yağış su bas...</td>\n",
       "      <td>Ordu'nun Fatsa ilçesinde etkili olan şiddetli ...</td>\n",
       "      <td>Gündem</td>\n",
       "      <td>Hayati Akçay</td>\n",
       "      <td>08.07.2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  İlk Türk astronot Gezeravcı'nın da yer aldığı ...   \n",
       "1  Ordu'nun Fatsa ilçesinde şiddetli yağış su bas...   \n",
       "\n",
       "                                             content      category  \\\n",
       "0  Türk, İspanyol-Amerikan, İtalyan ve İsveçli ol...  Gündem-Dünya   \n",
       "1  Ordu'nun Fatsa ilçesinde etkili olan şiddetli ...        Gündem   \n",
       "\n",
       "                           author        date  \n",
       "0  Zeynep Katre Oran, Irmak Akcan  2024.01.20  \n",
       "1                    Hayati Akçay  08.07.2024  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.DataFrame(data, columns=['title', 'content', 'category', 'author', 'date'])\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d850f4dffe9f4410a724176f02cf8314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news_list = news_data.content.tolist()\n",
    "news_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences = news_list,\n",
    "            api_calls_per_second = 20/60, \n",
    "            batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('news_embeddings.pkl', 'wb') as file:\n",
    "#     pickle.dump(news_embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06223642  0.00290349  0.00538724 ...  0.00327592 -0.02683627\n",
      "  -0.00044584]\n",
      " [-0.0238795   0.00681837 -0.01852706 ... -0.0016953  -0.00509094\n",
      "  -0.01097084]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('news_embeddings.pkl', 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    news_embeddings = pickle.load(file)\n",
    "  \n",
    "    print(news_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>İlk Türk astronot Gezeravcı'nın da yer aldığı ...</td>\n",
       "      <td>Türk, İspanyol-Amerikan, İtalyan ve İsveçli ol...</td>\n",
       "      <td>Gündem-Dünya</td>\n",
       "      <td>Zeynep Katre Oran, Irmak Akcan</td>\n",
       "      <td>2024.01.20</td>\n",
       "      <td>[-0.06223641708493233, 0.002903488464653492, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ordu'nun Fatsa ilçesinde şiddetli yağış su bas...</td>\n",
       "      <td>Ordu'nun Fatsa ilçesinde etkili olan şiddetli ...</td>\n",
       "      <td>Gündem</td>\n",
       "      <td>Hayati Akçay</td>\n",
       "      <td>08.07.2024</td>\n",
       "      <td>[-0.023879501968622208, 0.0068183704279363155,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  İlk Türk astronot Gezeravcı'nın da yer aldığı ...   \n",
       "1  Ordu'nun Fatsa ilçesinde şiddetli yağış su bas...   \n",
       "\n",
       "                                             content      category  \\\n",
       "0  Türk, İspanyol-Amerikan, İtalyan ve İsveçli ol...  Gündem-Dünya   \n",
       "1  Ordu'nun Fatsa ilçesinde etkili olan şiddetli ...        Gündem   \n",
       "\n",
       "                           author        date  \\\n",
       "0  Zeynep Katre Oran, Irmak Akcan  2024.01.20   \n",
       "1                    Hayati Akçay  08.07.2024   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.06223641708493233, 0.002903488464653492, 0...  \n",
       "1  [-0.023879501968622208, 0.0068183704279363155,...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data[\"embeddings\"] = news_embeddings.tolist()\n",
    "news_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances_argmin as distances_argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['space']\n",
    "query_embedding = embedding_model.get_embeddings(query)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_array = cosine_similarity([query_embedding],\n",
    "                                  list(news_data.embeddings.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_doc_cosine = np.argmax(cos_sim_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_doc_distances = distances_argmin([query_embedding], \n",
    "                                       list(news_data.embeddings.values))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Türk, İspanyol-Amerikan, İtalyan ve İsveçli olmak üzere çok uluslu Ax-3 ekibini taşıyan Dragon kapsülünün alçak dünya yörüngesindeki laboratuvar olarak tanımlanan UUİ'ye kenetlenme anı, Axiom Space'in internet sitesinden canlı yayınlandı.\\nYaklaşık 36 saat süren uzay yolculuğunun ardından ilk Türk astronot Gezeravcı'nın da yer aldığı Ax-3 ekibi, ABD yerel saatiyle sabah 05.42 (TSİ 13.42) civarında UUİ'ye ulaşt\\nUUİ'de bulunan, misyonları 27 Eylül 2023'te başlayan Expedition 70 astronot ve kozmonotları , Ax-3 ekibini, istasyonda karşıladı\\nGezeravcı, 2 hafta boyunca uzayda bilimsel çalışmalar yapacak\\nGezeravcı, 14 gün boyunca kalacağı UUİ'de 13 farklı bilimsel deney üzerinde çalışacak.\\nBu deneyler, mikro yer çekimi, uzay ortamında insan sağlığı, Tuz Gölü bitkisinin uzay ortamında araştırılması, katı-akışkan karışımların yerçekimsiz ortamda araştırılması gibi alanlarda çeşitlilik gösteriyor.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.content[index_doc_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Türk, İspanyol-Amerikan, İtalyan ve İsveçli olmak üzere çok uluslu Ax-3 ekibini taşıyan Dragon kapsülünün alçak dünya yörüngesindeki laboratuvar olarak tanımlanan UUİ'ye kenetlenme anı, Axiom Space'in internet sitesinden canlı yayınlandı.\\nYaklaşık 36 saat süren uzay yolculuğunun ardından ilk Türk astronot Gezeravcı'nın da yer aldığı Ax-3 ekibi, ABD yerel saatiyle sabah 05.42 (TSİ 13.42) civarında UUİ'ye ulaşt\\nUUİ'de bulunan, misyonları 27 Eylül 2023'te başlayan Expedition 70 astronot ve kozmonotları , Ax-3 ekibini, istasyonda karşıladı\\nGezeravcı, 2 hafta boyunca uzayda bilimsel çalışmalar yapacak\\nGezeravcı, 14 gün boyunca kalacağı UUİ'de 13 farklı bilimsel deney üzerinde çalışacak.\\nBu deneyler, mikro yer çekimi, uzay ortamında insan sağlığı, Tuz Gölü bitkisinin uzay ortamında araştırılması, katı-akışkan karışımların yerçekimsiz ortamda araştırılması gibi alanlarda çeşitlilik gösteriyor.\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.content[index_doc_distances]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question answering with relevant context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\n",
    "    \"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Uzay uçusu ne zaman oldu'\n",
    "query_embedding = embedding_model.get_embeddings(query)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"News: \" + news_data.content[index_doc_cosine] +\\\n",
    "\"\\n Title: \" + news_data.title[index_doc_cosine] +\\\n",
    "\"\\n Date:\" + news_data.date[index_doc_cosine] +\\\n",
    "\"\\n Category: \" + news_data.category[index_doc_cosine] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Here is the context: {context}\n",
    "             Give me response in Turkish.\n",
    "             Using the relevant information from the context,\n",
    "             provide an answer to the query: {query}.\"\n",
    "             If the context doesn't provide \\\n",
    "             any relevant information, \\\n",
    "             answer with \\\n",
    "             [I couldn't find a good match in the \\\n",
    "             document database for your query]\n",
    "             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Uzay uçuşu 20 Ocak 2024'te gerçekleşti."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "t_value = 0.2\n",
    "response = generation_model.predict(prompt = prompt,\n",
    "                                    temperature = t_value,\n",
    "                                    max_output_tokens = 1024)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I couldn't find a good match in the document database for your query."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = ['How to make the perfect lasagna']\n",
    "query_embedding = embedding_model.get_embeddings(query)[0].values\n",
    "\n",
    "cos_sim_array = cosine_similarity([query_embedding], \n",
    "                                  list(news_data.embeddings.values))\n",
    "index_doc = np.argmax(cos_sim_array)\n",
    "\n",
    "context = \"News: \" + news_data.content[index_doc_cosine] +\\\n",
    "\"\\n Title: \" + news_data.title[index_doc_cosine] +\\\n",
    "\"\\n Date:\" + news_data.date[index_doc_cosine] +\\\n",
    "\"\\n Category: \" + news_data.category[index_doc_cosine] \n",
    "\n",
    "prompt = f\"\"\"Here is the context: {context}\n",
    "             Give me response in Turkish.\n",
    "             Using the relevant information from the context,\n",
    "             provide an answer to the query: {query}.\"\n",
    "             If the context doesn't provide \\\n",
    "             any relevant information, \\\n",
    "             answer with \\\n",
    "             [I couldn't find a good match in the \\\n",
    "             document database for your query]\n",
    "             \"\"\"\n",
    "\n",
    "t_value = 0.2\n",
    "response = generation_model.predict(prompt = prompt,\n",
    "                                    temperature = t_value,\n",
    "                                    max_output_tokens = 1024)\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(embedded_dataset, \n",
    "                 num_leaves,\n",
    "                 num_leaves_to_search,\n",
    "                 training_sample_size):\n",
    "    \n",
    "    # normalize data to use cosine sim as explained in the paper\n",
    "    normalized_dataset = embedded_dataset / np.linalg.norm(embedded_dataset, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    searcher = (\n",
    "        scann.scann_ops_pybind.builder(normalized_dataset, 10, \"dot_product\")\n",
    "        .tree(\n",
    "            num_leaves = num_leaves,\n",
    "            num_leaves_to_search = num_leaves_to_search,\n",
    "            training_sample_size = training_sample_size,\n",
    "        )\n",
    "        .score_ah(2, anisotropic_quantization_threshold = 0.2)\n",
    "        .reorder(100)\n",
    "        .build()\n",
    "    )\n",
    "    return searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale with approximate nearest neighbor search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scann\n",
    "\n",
    "#Create index using scann\n",
    "index = create_index(embedded_dataset = question_embeddings, \n",
    "                     num_leaves = 25,\n",
    "                     num_leaves_to_search = 10,\n",
    "                     training_sample_size = 2000)\n",
    "\n",
    "query = \"how to concat dataframes pandas\"\n",
    "\n",
    "import time \n",
    "\n",
    "start = time.time()\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "neighbors, distances = index.search(query_embedding, final_num_neighbors = 1)\n",
    "end = time.time()\n",
    "\n",
    "for id, dist in zip(neighbors, distances):\n",
    "    print(f\"[docid:{id}] [{dist}] -- {so_database.input_text[int(id)][:125]}...\")\n",
    "\n",
    "print(\"Latency (ms):\", 1000 * (end - start))\n",
    "\n",
    "start = time.time()\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "cos_sim_array = cosine_similarity([query_embedding], list(so_database.embeddings.values))\n",
    "index_doc = np.argmax(cos_sim_array)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"[docid:{index_doc}] [{np.max(cos_sim_array)}] -- {so_database.input_text[int(index_doc)][:125]}...\")\n",
    "\n",
    "print(\"Latency (ms):\", 1000 * (end - start))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Mar 25 2020, 17:03:02) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e68f48f757f27e07250b73c6151e4820dd2d9b0a7c4f853122bf96447b625da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
